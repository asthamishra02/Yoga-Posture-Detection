{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcbf12-eb4e-4e9c-97bd-14bf9f8bc9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe pose class\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Initialize drawing class\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open a video file or camera stream\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for webcam, or provide video path\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform pose detection\n",
    "    result = pose.process(image_rgb)\n",
    "\n",
    "    # Draw pose landmarks on the frame\n",
    "    mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Pose Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e0c78-3575-4a60-92bc-b3e6291228af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    # Extract x, y coordinates of landmarks\n",
    "    keypoints = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten()\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326a392-a7bf-4919-a178-afea6c4bd3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "keypoints_list = []\n",
    "\n",
    "# For each image\n",
    "for image, label in dataset:\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    keypoints = extract_keypoints(results)\n",
    "    keypoints_list.append(keypoints)\n",
    "    labels.append(label)\n",
    "    \n",
    "# Convert to numpy arrays\n",
    "X = np.array(keypoints_list)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8ae2e-52dc-4e5d-816c-ec35e52b1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Build a simple neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7106d7-a68c-452d-b1dd-6e2ec1d16ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time yoga posture detection\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Pose detection\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(image_rgb)\n",
    "\n",
    "    # Extract keypoints\n",
    "    if result.pose_landmarks:\n",
    "        keypoints = extract_keypoints(result)\n",
    "\n",
    "        # Reshape and predict\n",
    "        keypoints = np.expand_dims(keypoints, axis=0)\n",
    "        prediction = model.predict(keypoints)\n",
    "        predicted_pose = np.argmax(prediction)\n",
    "\n",
    "        # Display the result on the frame\n",
    "        cv2.putText(frame, predicted_pose_name, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Yoga Posture Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d21e06-6b80-4f15-abfa-039e0530fe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\astha\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m result \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(image_rgb)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Draw pose landmarks on the frame\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(frame, result\u001b[38;5;241m.\u001b[39mpose_landmarks, mp_pose\u001b[38;5;241m.\u001b[39mPOSE_CONNECTIONS)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Display the frame\u001b[39;00m\n\u001b[0;32m     29\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPose Detection\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mediapipe\\python\\solutions\\drawing_utils.py:195\u001b[0m, in \u001b[0;36mdraw_landmarks\u001b[1;34m(image, landmark_list, connections, landmark_drawing_spec, connection_drawing_spec, is_drawing_landmarks)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# White circle border\u001b[39;00m\n\u001b[0;32m    193\u001b[0m circle_border_radius \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(drawing_spec\u001b[38;5;241m.\u001b[39mcircle_radius \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    194\u001b[0m                            \u001b[38;5;28mint\u001b[39m(drawing_spec\u001b[38;5;241m.\u001b[39mcircle_radius \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.2\u001b[39m))\n\u001b[1;32m--> 195\u001b[0m cv2\u001b[38;5;241m.\u001b[39mcircle(image, landmark_px, circle_border_radius, WHITE_COLOR,\n\u001b[0;32m    196\u001b[0m            drawing_spec\u001b[38;5;241m.\u001b[39mthickness)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Fill color into the circle\u001b[39;00m\n\u001b[0;32m    198\u001b[0m cv2\u001b[38;5;241m.\u001b[39mcircle(image, landmark_px, drawing_spec\u001b[38;5;241m.\u001b[39mcircle_radius,\n\u001b[0;32m    199\u001b[0m            drawing_spec\u001b[38;5;241m.\u001b[39mcolor, drawing_spec\u001b[38;5;241m.\u001b[39mthickness)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe pose class\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Initialize drawing class\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open a video file or camera stream\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for webcam, or provide video path\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform pose detection\n",
    "    result = pose.process(image_rgb)\n",
    "\n",
    "    # Draw pose landmarks on the frame\n",
    "    mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Pose Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "#close button enable\n",
    "#screen size fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff3c00-95fa-46b7-bf66-30ed41712891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\astha\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Holistic model (captures pose and hands only, no face)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(static_image_mode=False, model_complexity=1, smooth_landmarks=True, enable_segmentation=False, refine_face_landmarks=False)\n",
    "\n",
    "# Initialize drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open video capture (0 for webcam, or provide a video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get default video width and height\n",
    "screen_width = int(cap.get(3))\n",
    "screen_height = int(cap.get(4))\n",
    "\n",
    "cv2.namedWindow(\"Full-Body Detection\", cv2.WINDOW_NORMAL)  # Allows resizing\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process image with holistic model\n",
    "    result = holistic.process(image_rgb)\n",
    "\n",
    "    # Draw landmarks for pose and hands (Face is completely excluded)\n",
    "    if result.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    if result.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    if result.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "    # Resize frame to fit screen properly when maximized\n",
    "    frame_resized = cv2.resize(frame, (screen_width, screen_height))\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Full-Body Detection\", frame_resized)\n",
    "    \n",
    "    # Allow fullscreen mode when maximized\n",
    "    cv2.setWindowProperty(\"Full-Body Detection\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc886a6-802e-4626-9bca-cf13a953ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(\n",
    "    static_image_mode=False, model_complexity=1, smooth_landmarks=True,\n",
    "    enable_segmentation=False, refine_face_landmarks=False\n",
    ")\n",
    "\n",
    "# Initialize drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "cv2.namedWindow(\"Full-Body Detection\", cv2.WINDOW_NORMAL)  # Allows resizing\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process image with holistic model\n",
    "    result = holistic.process(image_rgb)\n",
    "\n",
    "    # Draw landmarks for pose and hands\n",
    "    if result.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    if result.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    if result.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Full-Body Detection\", frame)\n",
    "\n",
    "    # Handle window close event\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "        break\n",
    "    if cv2.getWindowProperty(\"Full-Body Detection\", cv2.WND_PROP_VISIBLE) < 1:  # Window closed\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adb4799-2677-4802-b9af-79f79b68290c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
